{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a117455",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/guswns3396/PSYC-221-Project/blob/main/Segment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xw9eZqktN53Q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xw9eZqktN53Q",
    "outputId": "8b1a6676-fc0f-4fff-e415-b8ae2317b170"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hpRHy7MQAH0q",
   "metadata": {
    "id": "hpRHy7MQAH0q"
   },
   "source": [
    "# Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cgz1UzGxAAZp",
   "metadata": {
    "id": "cgz1UzGxAAZp"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "import monai\n",
    "import monai.transforms as mt\n",
    "from monai.data import PersistentDataset, Dataset, DataLoader, decollate_batch\n",
    "from monai.apps import CrossValidation\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jpWj_qLSi6Mc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpWj_qLSi6Mc",
    "outputId": "a1860d10-b6a1-4672-f9ba-fb34ffea96a6"
   },
   "outputs": [],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GOzScJBwiNCA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOzScJBwiNCA",
    "outputId": "aeb5ff21-5c2b-459f-c1e9-57cf17124c77"
   },
   "outputs": [],
   "source": [
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fhC342sKE-Ee",
   "metadata": {
    "id": "fhC342sKE-Ee"
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eYXAWxZW10GM",
   "metadata": {
    "id": "eYXAWxZW10GM"
   },
   "outputs": [],
   "source": [
    "root_dir = '/home/users/yanghyun/psyc221'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GN3BAAIBGjT7",
   "metadata": {
    "id": "GN3BAAIBGjT7"
   },
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SEYlR-y-hEyu",
   "metadata": {
    "id": "SEYlR-y-hEyu"
   },
   "outputs": [],
   "source": [
    "IMG_TYPES = {\n",
    "  'bravo': 0,\n",
    "  'flair': 1,\n",
    "  't1_gd': 2,\n",
    "  't1_pre': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U76SpNPt0OZe",
   "metadata": {
    "id": "U76SpNPt0OZe"
   },
   "outputs": [],
   "source": [
    "def get_data_dicts(path):\n",
    "    data = [\n",
    "    {\n",
    "      'img': [os.path.join(subj, img_type + '.nii.gz') for img_type in IMG_TYPES],\n",
    "      'seg': os.path.join(subj, 'seg.nii.gz')\n",
    "    }\n",
    "    for subj in glob(os.path.join(path, 'Mets_*'))\n",
    "    ]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2xi1bmdx4HVf",
   "metadata": {
    "id": "2xi1bmdx4HVf"
   },
   "outputs": [],
   "source": [
    "def fix_meta(metatensor):\n",
    "    \"\"\"\n",
    "    fix meta information of metatensor after stacking\n",
    "    \"\"\"\n",
    "    # fix img meta\n",
    "    a = [metatensor.ndim, *metatensor.shape[1:], metatensor.shape[0]]\n",
    "    for i, val in enumerate(a):\n",
    "        metatensor.meta['dim'][i] = val\n",
    "        metatensor.meta['original_channel_dim'] = -1\n",
    "    return metatensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nSojSNFB-z4i",
   "metadata": {
    "id": "nSojSNFB-z4i"
   },
   "outputs": [],
   "source": [
    "def print_data(metatensor):\n",
    "    print(metatensor.shape)\n",
    "    print(metatensor.meta)\n",
    "    return metatensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29a7bbd",
   "metadata": {
    "id": "a29a7bbd"
   },
   "outputs": [],
   "source": [
    "path_train = os.path.join(root_dir, 'brainmetshare-3/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZjygWPYGz4h-",
   "metadata": {
    "id": "ZjygWPYGz4h-"
   },
   "outputs": [],
   "source": [
    "data_dicts = get_data_dicts(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jQNIxi3nCruN",
   "metadata": {
    "id": "jQNIxi3nCruN"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Z7HRPP-Y4I_-",
   "metadata": {
    "id": "Z7HRPP-Y4I_-"
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset(\n",
    "  data_dicts,\n",
    "  transform=mt.Compose([\n",
    "    # load images\n",
    "    mt.LoadImageD(['img', 'seg']),\n",
    "  ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5X-xZlPjyrRD",
   "metadata": {
    "id": "5X-xZlPjyrRD"
   },
   "outputs": [],
   "source": [
    "def get_imbalance(ds):\n",
    "    ones = 0\n",
    "    zeroes = 0\n",
    "    for d in tqdm(ds):\n",
    "        ones += (d['seg'] == 1).sum()\n",
    "        zeroes += (d['seg'] == 0).sum()\n",
    "    return zeroes / ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TQjIny76350_",
   "metadata": {
    "id": "TQjIny76350_"
   },
   "outputs": [],
   "source": [
    "# # look at imbalance of classes\n",
    "# ratio = get_imbalance(train_ds)\n",
    "# ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nmCZvNTj50Ll",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nmCZvNTj50Ll",
    "outputId": "77bfc3b5-59c0-4d21-d12c-9b91614c95ef"
   },
   "outputs": [],
   "source": [
    "ratio = torch.Tensor([2500])\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MMCIztiZ592a",
   "metadata": {
    "id": "MMCIztiZ592a"
   },
   "source": [
    "# Load & Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XpZnDnTiAWZP",
   "metadata": {
    "id": "XpZnDnTiAWZP"
   },
   "outputs": [],
   "source": [
    "KEYS = ('img', 'seg')\n",
    "k = 4 # divisible\n",
    "\n",
    "xform_train = mt.Compose([\n",
    "    # load images\n",
    "    mt.LoadImageD(KEYS),\n",
    "    # make channel the first dimension / add channel dimension if necessary\n",
    "    mt.EnsureChannelFirstD(KEYS),\n",
    "    # fix meta\n",
    "    mt.LambdaD(KEYS, fix_meta),\n",
    "    # print data to ensure fix\n",
    "    # mt.LambdaD(KEYS, print_data),\n",
    "    # make sure tensor type\n",
    "    mt.EnsureTypeD(keys=KEYS),\n",
    "    # make sure all have same orientation (axcode)\n",
    "    mt.Orientationd(keys=KEYS, axcodes=\"RAS\"),\n",
    "    mt.Spacingd(\n",
    "      keys=KEYS,\n",
    "      pixdim=(1.0, 1.0, 1.0),\n",
    "      mode=(\"bilinear\", \"nearest\"),\n",
    "    ),\n",
    "    # augment data to be invariant to orientation\n",
    "    # mt.RandFlipd(keys=KEYS, prob=0.5, spatial_axis=0),\n",
    "    # mt.RandFlipd(keys=KEYS, prob=0.5, spatial_axis=1),\n",
    "    # mt.RandFlipd(keys=KEYS, prob=0.5, spatial_axis=2),\n",
    "    # normalize intensity\n",
    "    mt.NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    # augment data to be invariant to intensity shift or scale\n",
    "    # mt.RandScaleIntensityd(keys=\"img\", factors=0.1, prob=0.1),\n",
    "    # mt.RandShiftIntensityd(keys=\"img\", offsets=0.1, prob=0.1),\n",
    "    # pad data to be divisible\n",
    "    mt.DivisiblePadD(keys=KEYS, k=k),\n",
    "])\n",
    "\n",
    "xform_val = mt.Compose([\n",
    "    # load images\n",
    "    mt.LoadImageD(KEYS),\n",
    "    # make channel the first dimension / add channel dimension if necessary\n",
    "    mt.EnsureChannelFirstD(KEYS),\n",
    "    # fix meta\n",
    "    mt.LambdaD(KEYS, fix_meta),\n",
    "    # print data to ensure fix\n",
    "    # mt.LambdaD(KEYS, print_data),\n",
    "    # make sure tensor type\n",
    "    mt.EnsureTypeD(keys=KEYS),\n",
    "    # make sure all have same orientation (axcode)\n",
    "    mt.Orientationd(keys=KEYS, axcodes=\"RAS\"),\n",
    "    mt.Spacingd(\n",
    "      keys=KEYS,\n",
    "      pixdim=(1.0, 1.0, 1.0),\n",
    "      mode=(\"bilinear\", \"nearest\"),\n",
    "    ),\n",
    "    # normalize intensity\n",
    "    mt.NormalizeIntensityd(keys=\"img\", nonzero=True, channel_wise=True),\n",
    "    # pad data to be divisible\n",
    "    mt.DivisiblePadD(keys=KEYS, k=k),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_9MfS7xPLSgl",
   "metadata": {
    "id": "_9MfS7xPLSgl"
   },
   "outputs": [],
   "source": [
    "# use PersistentDataset so deterministic preprocessing operations aren't repeated\n",
    "# slow initialization, fast retrieval\n",
    "# cache stored in disk not memory\n",
    "class MRIDataset(ABC, PersistentDataset):\n",
    "    \"\"\"\n",
    "    Base class to generate cross validation datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        transform,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        data = self._split_datalist(datalist=data)\n",
    "        super().__init__(data, transform, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _split_datalist(self, datalist):\n",
    "        raise NotImplementedError(f\"Subclass {self.__class__.__name__} must implement this method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qYPLBTXwLzwQ",
   "metadata": {
    "id": "qYPLBTXwLzwQ"
   },
   "outputs": [],
   "source": [
    "# k-fold cross validation\n",
    "num = 2\n",
    "folds = list(range(num))\n",
    "\n",
    "cache_dir = os.path.join(root_dir, 'cache_dir')\n",
    "\n",
    "cvdataset = CrossValidation(\n",
    "    dataset_cls=MRIDataset,\n",
    "    data=data_dicts[:2], # subset data for Colab\n",
    "    nfolds=num,\n",
    "    transform=xform_train,\n",
    "    cache_dir=cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hrEmxiH9ULRP",
   "metadata": {
    "id": "hrEmxiH9ULRP"
   },
   "outputs": [],
   "source": [
    "# clear cache\n",
    "for c in glob(os.path.join(cache_dir, '*')):\n",
    "    os.remove(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VIKbmsCjMhC0",
   "metadata": {
    "id": "VIKbmsCjMhC0"
   },
   "outputs": [],
   "source": [
    "num_workers=0\n",
    "batch_size=1\n",
    "\n",
    "# get datasets\n",
    "train_dss = [cvdataset.get_dataset(folds=folds[0:i] + folds[(i + 1) :]) for i in folds]\n",
    "val_dss = [cvdataset.get_dataset(folds=i, transform=xform_val) for i in folds]\n",
    "\n",
    "# get loaders & set batch size, number of workers, shuffle\n",
    "train_loaders = [DataLoader(train_dss[i], batch_size=batch_size, shuffle=True, num_workers=num_workers) for i in folds]\n",
    "val_loaders = [DataLoader(val_dss[i], batch_size=batch_size, num_workers=num_workers) for i in folds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WpXvqFmWDnOs",
   "metadata": {
    "id": "WpXvqFmWDnOs"
   },
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MpophQYg7raU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "MpophQYg7raU",
    "outputId": "c4ecabb3-6266-450d-d9f3-87b2d2cc48b0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds_idx = 0\n",
    "subj_idx = 0\n",
    "\n",
    "data = train_dss[ds_idx][subj_idx]\n",
    "slice_id = np.argmax(data['seg'][0].sum(axis=[0,1])) # slice with most metastases\n",
    "\n",
    "# visualize a slice from each image modality with segmentation overlay\n",
    "print(f\"subject: {data['img'].meta['filename_or_obj']}\")\n",
    "print(f\"image shape: {data['img'].shape}\")\n",
    "print(f\"segment shape: {data['seg'].shape}\")\n",
    "plt.figure(\"image\", (24, 6))\n",
    "for i, img_type in enumerate(IMG_TYPES):\n",
    "    plt.subplot(1, len(IMG_TYPES), i + 1)\n",
    "    plt.title(f\"image channel {img_type}\")\n",
    "    plt.imshow(data['img'][IMG_TYPES[img_type], :, :, slice_id].detach().cpu(),  cmap=\"gray\")\n",
    "    plt.imshow(data['seg'][0, :, :, slice_id].detach().cpu(), cmap='jet', alpha=0.2) # interpolation='none'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QaV_bKNv52qq",
   "metadata": {
    "id": "QaV_bKNv52qq"
   },
   "source": [
    "# Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O541sMjkDh1G",
   "metadata": {
    "id": "O541sMjkDh1G"
   },
   "outputs": [],
   "source": [
    "# class Spec:\n",
    "#   def __init__(self, constr, **kwargs):\n",
    "#     self.constr = constr\n",
    "#     self.kwargs = kwargs\n",
    "\n",
    "#   def __call__(self):\n",
    "#     raise NotImplementedError\n",
    "\n",
    "# class ModelSpec(Spec):\n",
    "#   def __init__(self, const, **kwargs):\n",
    "#     super().__init__(const, **kwargs)\n",
    "#   def __call__(self):\n",
    "#     return self.constr(**self.kwargs)\n",
    "\n",
    "# class LossSpec(Spec):\n",
    "#   def __init__(self, const, **kwargs):\n",
    "#     super().__init__(const, **kwargs)\n",
    "#   def __call__(self):\n",
    "#     return self.constr(**self.kwargs)\n",
    "\n",
    "# class OptSpec(Spec):\n",
    "#   def __init__(self, const, **kwargs):\n",
    "#     super().__init__(const, **kwargs)\n",
    "#   def __call__(self, params):\n",
    "#     return self.constr(params=params, **self.kwargs)\n",
    "\n",
    "# class SpecComb:\n",
    "#   def __init__(self, model_spec, loss_spec, opt_spec):\n",
    "#     self.model_spec = model_spec\n",
    "#     self.loss_spec = loss_spec\n",
    "#     self.opt_spec = opt_spec\n",
    "\n",
    "#   def __call__(self):\n",
    "#     model = self.model_spec().to(device)\n",
    "#     loss = self.loss_spec()\n",
    "#     opt = self.opt_spec(model.parameters())\n",
    "\n",
    "#     return model, loss, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EMIrmvsHLjFC",
   "metadata": {
    "id": "EMIrmvsHLjFC"
   },
   "outputs": [],
   "source": [
    "# spec_combs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1_1NiOa0D5V9",
   "metadata": {
    "id": "1_1NiOa0D5V9"
   },
   "outputs": [],
   "source": [
    "# spec_comb = SpecComb(\n",
    "#   ModelSpec(\n",
    "#       monai.networks.nets.UNet,\n",
    "#       # 3D\n",
    "#       spatial_dims=3,\n",
    "#       # 4 modalities\n",
    "#       in_channels=4,\n",
    "#       # 1 channel for output\n",
    "#       out_channels=1,\n",
    "#       # layers\n",
    "#       channels=(16, 32),\n",
    "#       strides=(2,),\n",
    "#       kernel_size=3,\n",
    "#       up_kernel_size=3,\n",
    "#       # num_res_units=2,\n",
    "#       act='PRELU',\n",
    "#       # norm=monai.networks.layers.Norm.BATCH,\n",
    "#       # dropout=0,\n",
    "#       # bias=True,\n",
    "#       adn_ordering='NDA'\n",
    "#   ),\n",
    "#   LossSpec(\n",
    "#       monai.losses.DiceLoss\n",
    "#   ),\n",
    "#   OptSpec(\n",
    "#       torch.optim.Adam,\n",
    "#       lr=1e-2\n",
    "#   )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HQaGuEgSLlE9",
   "metadata": {
    "id": "HQaGuEgSLlE9"
   },
   "outputs": [],
   "source": [
    "# spec_combs.append(spec_comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bmq4vd7M6AlY",
   "metadata": {
    "id": "Bmq4vd7M6AlY"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zufneUENWSo7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zufneUENWSo7",
    "outputId": "9df50854-052b-4464-b335-2ea857639b6c"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SQ9UJmvG06TN",
   "metadata": {
    "id": "SQ9UJmvG06TN"
   },
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "val_interval = 1 # validate every val_interval epochs\n",
    "save_interval = 5 # save checkpoint every save_interval epochs\n",
    "checkpoint_path = os.path.join(root_dir, 'model_checkpoint.pt')\n",
    "\n",
    "roi_size = (120, 120, 76)\n",
    "sw_batch_size = 4\n",
    "\n",
    "post_pred = mt.Compose([mt.Activations(sigmoid=True), mt.AsDiscrete(threshold=0.5)])\n",
    "post_label = mt.Compose([mt.AsDiscrete()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_t7mGQUJ_LHh",
   "metadata": {
    "id": "_t7mGQUJ_LHh"
   },
   "source": [
    "[decollate_batch](https://github.com/Project-MONAI/tutorials/blob/main/modules/decollate_batch.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aIyaH_YFfjur",
   "metadata": {
    "id": "aIyaH_YFfjur"
   },
   "outputs": [],
   "source": [
    "# configure network\n",
    "model = monai.networks.nets.UNet(\n",
    "    spatial_dims=3, # 3D\n",
    "    in_channels=4, # 4 modalities\n",
    "    out_channels=1, # 1 channel for output\n",
    "    channels=(16, 32), # layers\n",
    "    strides=(2,),\n",
    "    kernel_size=3,\n",
    "    up_kernel_size=3,\n",
    "    # num_res_units=2,\n",
    "    act='PRELU',\n",
    "    # norm=monai.networks.layers.Norm.BATCH,\n",
    "    # dropout=0,\n",
    "    # bias=True,\n",
    "    adn_ordering='NDA'\n",
    ").to(device)\n",
    "# set loss, optimizer, metric\n",
    "# loss_function = monai.losses.DiceCELoss(sigmoid=True, weight=ratio)\n",
    "loss_function = torch.nn.BCEWithLogitsLoss(weight=ratio)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "metric_function = monai.metrics.DiceMetric(include_background=False, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lJ1WSJ_7FDnB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJ1WSJ_7FDnB",
    "outputId": "02ea94b3-4a32-4418-a9ef-170db5d47e97"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Ib-aCEmFAxH",
   "metadata": {
    "id": "8Ib-aCEmFAxH"
   },
   "outputs": [],
   "source": [
    "# initialize variables for tracking\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    # set to train mode\n",
    "    model.train()\n",
    "    # initialize loss\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    # batch train\n",
    "    for batch_data in train_loaders[0]:\n",
    "        step += 1\n",
    "        # load data & move to device\n",
    "        inputs, labels = (\n",
    "            batch_data[\"img\"].to(device),\n",
    "            batch_data[\"seg\"].to(device),\n",
    "        )\n",
    "        # zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = model(inputs)\n",
    "        # get loss\n",
    "        loss = loss_function(outputs, labels)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # running loss for epoch\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_dss[0]) // train_loaders[0].batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
    "    # average loss over batches for epoch\n",
    "    epoch_loss /= step\n",
    "    # track epoch losses\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # save checkpoint\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "    # validate\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        # set model to evaulation mode\n",
    "        model.eval()\n",
    "        # disable gradiant computation\n",
    "        with torch.no_grad():\n",
    "            # iterate over validation data\n",
    "            for val_data in val_loaders[0]:\n",
    "                # load validation data\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"img\"].to(device),\n",
    "                    val_data[\"seg\"].to(device),\n",
    "                )\n",
    "                # get output using sliding window inference\n",
    "                val_outputs = sliding_window_inference(val_inputs, roi_size, sw_batch_size, model, overlap=0.25)\n",
    "                # turn batch into list of images\n",
    "                val_outputs = [post_pred(i) for i in decollate_batch(val_outputs)]\n",
    "                val_labels = [post_label(i) for i in decollate_batch(val_labels)]\n",
    "                # compute metric for current iteration\n",
    "                metric_function(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            # aggregate the final mean dice result\n",
    "            metric = metric_function.aggregate().item()\n",
    "            # reset the status for next validation round\n",
    "            metric_function.reset()\n",
    "            # track final metric result\n",
    "            metric_values.append(metric)\n",
    "\n",
    "            # update model if better performance\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
    "                f\"at epoch: {best_metric_epoch}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1REmIFqG4hdL",
   "metadata": {
    "id": "1REmIFqG4hdL"
   },
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {best_metric:.4f} \" f\"at epoch: {best_metric_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJIXa03QV1vJ",
   "metadata": {
    "id": "MJIXa03QV1vJ"
   },
   "outputs": [],
   "source": [
    "# Delete cache\n",
    "import shutil\n",
    "shutil.rmtree(cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PdeoOhdV2Lw",
   "metadata": {
    "id": "7PdeoOhdV2Lw"
   },
   "source": [
    "# Plot Loss & Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fab1bik8O8yv",
   "metadata": {
    "id": "Fab1bik8O8yv"
   },
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rVOwXOQbOlws",
   "metadata": {
    "id": "rVOwXOQbOlws"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4jGHQDlPcS8",
   "metadata": {
    "id": "e4jGHQDlPcS8"
   },
   "outputs": [],
   "source": [
    "input = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nwK7IQ2EWRyn",
   "metadata": {
    "id": "nwK7IQ2EWRyn"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input = input.to(device)\n",
    "    output = sliding_window_inference(input, roi_size, sw_batch_size, model, overlap=0.25)\n",
    "    output = post_pred(output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
